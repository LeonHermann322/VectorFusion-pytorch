seed: 1
image_size: 600
path_svg: ~  # if you want to load a svg file and train from it
skip_live: False
style: "iconography" # "pixelart", "sketch"

# train
batch_size: 1
num_iter: 500 # num_iter per path group
num_stages: 1 # training stages, you can train x strokes, then freeze them and train another x strokes etc
lr_base:
  point: 1
  fill_color: 0.01
  bg: 0.01
  stroke_width: 0.1
  stroke_color: 0.01
lr_scheduler: True

# primitives
num_paths: 128 # number of strokes
path_schedule: 'repeat' # 'list'
schedule_each: 16 # [1, 3, 5, 7]
train_stroke: False # train stroke width and color
trainable_bg: False # set the background to be trainable
width: 3 # stroke width
num_segments: 4
segment_init: 'circle' # 'random'
radius: 20
coord_init: 'sparse' # 'random', 'naive', place the first control point
grid: 32 # divide the canvas into n grids
path_reinit: # reinitializing paths
  use: True
  freq: 50 # every 50 iterations
  stop_step: 800 # for SDS fine-tuning
  opacity_threshold: 0.05
  area_threshold: 64

# diffusion
model_id: "sd15" # sd14, sd15, sd21, sd21b, sdxl
ldm_speed_up: False
enable_xformers: False
gradient_checkpoint: False
num_inference_steps: 50
guidance_scale: 7.5 # sdxl default 5.0
K: 20
lora_path: ~

# SDS
sds:
  im_size: 512
  guidance_scale: 100
  grad_scale: 1.0
  t_range: [ 0.05, 0.95 ]
  num_iter: 1000 # fine-tuning steps

# Live loss
use_distance_weighted_loss: True
xing_loss_weight: 0.01
# pixel loss
penalty_weight: 0.05

# JVSP
use_jvsp: True
clip:
  model_name: "RN101"  # RN101, ViT-L/14
  feats_loss_type: "l2" # clip visual loss type, conv layers
  feats_loss_weights: [ 0,0,1.0,1.0,0 ] # RN based
  #  feats_loss_weights: [ 0,0,1.0,1.0,0,0,0,0,0,0,0,0 ] # ViT based
  fc_loss_weight: 0.1 # clip visual loss, fc layer weight
  augmentations: "affine" # augmentation before clip visual computation
  num_aug: 4 # num of augmentation before clip visual computation
  vis_loss: 1 # 1 or 0 for use or disable clip visual loss
  text_visual_coeff: 0 # cosine similarity between text and img
perceptual:
  name: "lpips" # dists
  lpips_net: 'vgg'
  coeff: 1.0
  
token_ind: 1 # the index of CLIP prompt embedding, start from 1, 0 is start token
attention_init: True # if True, use the attention heads of Dino model to set the location of the initial strokes
xdog_intersec: True # initialize along the edge, mix XDoG and attn up
softmax_temp: 0.5
cross_attn_res: 16
self_attn_res: 32
max_com: 20 # select the number of the self-attn maps
mean_comp: False # the average of the self-attn maps
comp_idx: 0 # if mean_comp==False, indicates the index of the self-attn map
attn_coeff: 1.0 # attn fusion, w * cross-attn + (1-w) * self-attn
log_cross_attn: False # True if cross attn every step
u2net_path: "./checkpoint/u2net/u2net.pth"

# Reverse diffusion and prompt
reverse_diffusion_mode: "finetuning" # Choices: "sampling", "finetuning", "none"
reverse_diffusion_embedding_mode: "interpolate" # Choices: "hard_cut", "interpolated", is only evaluated if mode is finetuning
attribute_importance: 0.3 # should be between 0.0 and 1.0
